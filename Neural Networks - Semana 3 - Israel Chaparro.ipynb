{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "#np.random.seed(0)\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'israel', ':)', ':)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test Israel! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "# Exercise 1: define features based on word embeddings (pre-trained word2vec / Glove/Fastext emebddings can be used)\n",
    "# Define suitable d dimension, and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  0.0 %\n",
      "Completed  20.0 %\n",
      "Completed  40.0 %\n",
      "Completed  60.0 %\n",
      "Completed  80.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Primero creamos el corpus y lo almacenamos\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "n = 50000\n",
    "corpus_words = []\n",
    "for i in range(n):\n",
    "    corpus_words += [tokenizer(df.iloc[i]['review'])]\n",
    "    if i %10000 == 0:\n",
    "        print(\"Completed \",20*i/10000,\"%\")  \n",
    "print(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Luego generamos el modelo usando word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(corpus_words,size=100)\n",
    "#print(model)\n",
    "w2v = dict(zip(model.wv.index2word,model.wv.vectors))\n",
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 100)\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]] (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Creamos un vector de medias de los embeddings\n",
    "def MeanEmbeddingVectorizer(tokenized_review):\n",
    "    mean_vector = []\n",
    "    for word in tokenized_review:\n",
    "        if word in w2v:\n",
    "            mean_vector.append(w2v[word].tolist())\n",
    "        else:\n",
    "            mean_vector.append(np.zeros(100))\n",
    "    mean_vector = np.mean(mean_vector,axis = 0)\n",
    "    #print(mean_vector)\n",
    "    return mean_vector\n",
    "\n",
    "# Actualizamos la data con la pre-procesada\n",
    "data = []\n",
    "for i in range(50000):\n",
    "    data.append(MeanEmbeddingVectorizer(corpus_words[i]))\n",
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "\n",
    "target = np.array([df['sentiment']])\n",
    "\n",
    "target = target.T\n",
    "print(target[:5],target.shape) # Verificamos correspondencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos la data en entrenaiento y test\n",
    "training_data = data[:45000]\n",
    "training_target = target[:45000]\n",
    "\n",
    "test_data = data[45000:]\n",
    "test_target = target[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programamos la red neuronal\n",
    "def sigmoid(x,derivative = False):\n",
    "    if derivative:\n",
    "        return sigmoid(x)*(1 - sigmoid(x))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\" Funcion para calcular la derivada del sigmoide cuando x es sig(x)\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "def evalPrediction(x,threshold,target):\n",
    "    \"\"\"Funcion que evalua el acierto de una salida, Devuelve un booleano\"\"\"\n",
    "    if x > threshold:\n",
    "        return target == 1\n",
    "    else:\n",
    "        return target == 0\n",
    "        \n",
    "sig_PrimeVectorized = np.vectorize(sigmoid_prime)\n",
    "sigVectorized = np.vectorize(sigmoid)\n",
    "evalPredictionVectorized = np.vectorize(evalPrediction)\n",
    "\n",
    "class Weights(object):\n",
    "    def __init__(self, numFeatures,numNeurons):\n",
    "        self.numFeatures =  numFeatures\n",
    "        self.numNeurons = numNeurons\n",
    "        self.weights = np.random.random((numFeatures,numNeurons))\n",
    "    \n",
    "    def printWeights(self):\n",
    "        print(self.weights)\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self,neurons,data,target,_test_data,_test_target):\n",
    "        # We include the bias at the start of data matrix\n",
    "        self.training_data = data\n",
    "        self.n = data.shape[0] #Numero de ejemplos en el training\n",
    "        self. numFeatures = self.training_data.shape[1]\n",
    "        self.training_target = target\n",
    "        # Input Layer\n",
    "        self.inLayer = None\n",
    "        # First Weight Matrix(nro col = nro de neuronas)\n",
    "        self.neuronsInHiddenLayer = neurons\n",
    "        # acordemonos que un feature adicional seria el bias\n",
    "        self.weights_1 = np.random.random((self.numFeatures+1,self.neuronsInHiddenLayer))\n",
    "        # Hidden Layers\n",
    "        self.hiddenLayer = None        \n",
    "        # Second Weight Matrix, Agregar una columna para el bias\n",
    "        self.weights_2 = np.random.random((self.neuronsInHiddenLayer + 1,1)) # Solo una neurona de salida\n",
    "        # Output Layer\n",
    "        self.outLayer = None\n",
    "        self.Layers = None\n",
    "        \n",
    "        # Loading Test Data, (Agregamos una fila de 1 para el bias)\n",
    "        _test_data = np.append( np.full((len(_test_data),1),1),_test_data,axis = 1)\n",
    "        self.test_data = _test_data\n",
    "        self.test_target = _test_target\n",
    "        \n",
    "        ## Labels para imprimir los arreglos\n",
    "        self.enum = {\n",
    "                0:\"Input Data:\\n\",\n",
    "                1:\"Weights 1:\\n\",\n",
    "                2:\"Hidden Layer:\\n\",\n",
    "                3:\"Weights 2:\\n\",\n",
    "                4:\"Output Layer:\\n\",}\n",
    "        \n",
    "    def forwardPropagation(self,i):\n",
    "        # Seleccionamos la cantidad de datos\n",
    "        self.inLayer = np.array([self.training_data[i%self.n]])\n",
    "        #self.inLayer = np.array(self.training_data)\n",
    "        self.inLayer = np.append( np.full((len(self.inLayer),1),1),self.inLayer,axis = 1)\n",
    "        \n",
    "        self.hiddenLayer = np.dot(self.inLayer,self.weights_1)\n",
    "        self.hiddenLayer = sigmoid(self.hiddenLayer)\n",
    "        self.hiddenLayer = np.append( np.full((len(self.hiddenLayer),1),1),self.hiddenLayer,axis = 1)\n",
    "        \n",
    "        self.outLayer = np.dot(self.hiddenLayer,self.weights_2)\n",
    "        self.outLayer = sigmoid(self.outLayer)\n",
    "        \n",
    "        self.updateLayers()\n",
    "        \n",
    "    def updateLayers(self):\n",
    "        self.Layers = [self.inLayer,self.weights_1,self.hiddenLayer,self.weights_2,self.outLayer]\n",
    "        \n",
    "    def backPropagation(self,i,learning_rate):\n",
    "        error = (self.outLayer[0][0] - self.training_target[i%self.n])\n",
    "        weights_1 = self.weights_1\n",
    "        weights_2 = self.weights_2\n",
    "        weights_2 = weights_2 - learning_rate * error * self.hiddenLayer.T\n",
    "        hidden = self.hiddenLayer.T \n",
    "        hidden = np.delete(hidden,(0),axis = 0)\n",
    "        hidden = sig_PrimeVectorized(hidden).T\n",
    "        derivative = error * np.dot(np.array([self.inLayer[0]]).T,hidden)\n",
    "        w2 = self.weights_2\n",
    "        w2 = np.delete(w2,(0),axis=0)\n",
    "        w2 = np.tile(w2,self.neuronsInHiddenLayer)\n",
    "        derivative = np.dot(derivative,w2)\n",
    "        weights_1 = weights_1 - learning_rate * derivative\n",
    "        self.weights_1 = weights_1\n",
    "        self.weights_2 = weights_2\n",
    "        self.updateLayers()\n",
    "\n",
    "    def Train(self,iterations,learn_rate):\n",
    "        alpha = learn_rate\n",
    "        b= True\n",
    "        for i in range(iterations):\n",
    "            self.forwardPropagation(i)\n",
    "            self.backPropagation(i,alpha)\n",
    "            #self.printNN()\n",
    "            if(i % 10000 == 0):\n",
    "                print(\"Progress:\",i / 10000,\"%\")\n",
    "                if(self.getAccuracy() > 70 and b):\n",
    "                    alpha /= 10\n",
    "                    b = False\n",
    "        self.printNN()\n",
    "        \n",
    "    def printNN(self):\n",
    "        print(\"#################\")\n",
    "        i = 0\n",
    "        for layer in self.Layers:\n",
    "            print(self.enum[i],layer)\n",
    "            i +=1\n",
    "        print(\"#################\")\n",
    "    def getAccuracy(self):\n",
    "        prediction = np.dot(self.test_data,self.weights_1)\n",
    "        prediction = sigmoid(prediction)\n",
    "        prediction = np.append( np.full((len(prediction),1),1),prediction,axis = 1)\n",
    "        prediction = np.dot(prediction,self.weights_2)\n",
    "        prediction = sigmoid(prediction)\n",
    "        threshold = 0.5      \n",
    "        xs = evalPredictionVectorized(prediction,threshold,self.test_target)\n",
    "        numSuccesses = np.count_nonzero(xs == True)\n",
    "        print(\"Acc:\", numSuccesses / len(xs) * 100)\n",
    "        return numSuccesses / len(xs) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0 %\n",
      "Acc: 50.63999999999999\n",
      "Progress: 1.0 %\n",
      "Acc: 78.97999999999999\n",
      "Progress: 2.0 %\n",
      "Acc: 80.46\n",
      "Progress: 3.0 %\n",
      "Acc: 81.86\n",
      "Progress: 4.0 %\n",
      "Acc: 82.96\n",
      "Progress: 5.0 %\n",
      "Acc: 83.48\n",
      "Progress: 6.0 %\n",
      "Acc: 83.78\n",
      "Progress: 7.0 %\n",
      "Acc: 83.96000000000001\n",
      "Progress: 8.0 %\n",
      "Acc: 84.46000000000001\n",
      "Progress: 9.0 %\n",
      "Acc: 84.38\n",
      "Progress: 10.0 %\n",
      "Acc: 84.89999999999999\n",
      "Progress: 11.0 %\n",
      "Acc: 85.02\n",
      "Progress: 12.0 %\n",
      "Acc: 85.24000000000001\n",
      "Progress: 13.0 %\n",
      "Acc: 85.34\n",
      "Progress: 14.0 %\n",
      "Acc: 85.44\n",
      "Progress: 15.0 %\n",
      "Acc: 85.52\n",
      "Progress: 16.0 %\n",
      "Acc: 85.72\n",
      "Progress: 17.0 %\n",
      "Acc: 85.86\n",
      "Progress: 18.0 %\n",
      "Acc: 85.84\n",
      "Progress: 19.0 %\n",
      "Acc: 85.86\n",
      "Progress: 20.0 %\n",
      "Acc: 86.06\n",
      "Progress: 21.0 %\n",
      "Acc: 86.0\n",
      "Progress: 22.0 %\n",
      "Acc: 86.08\n",
      "Progress: 23.0 %\n",
      "Acc: 86.18\n",
      "Progress: 24.0 %\n",
      "Acc: 86.14\n",
      "Progress: 25.0 %\n",
      "Acc: 86.2\n",
      "Progress: 26.0 %\n",
      "Acc: 86.3\n",
      "Progress: 27.0 %\n",
      "Acc: 86.22\n",
      "Progress: 28.0 %\n",
      "Acc: 86.32\n",
      "Progress: 29.0 %\n",
      "Acc: 86.22\n",
      "Progress: 30.0 %\n",
      "Acc: 86.42\n",
      "Progress: 31.0 %\n",
      "Acc: 86.4\n",
      "Progress: 32.0 %\n",
      "Acc: 86.36\n",
      "Progress: 33.0 %\n",
      "Acc: 86.38\n",
      "Progress: 34.0 %\n",
      "Acc: 86.36\n",
      "Progress: 35.0 %\n",
      "Acc: 86.42\n",
      "Progress: 36.0 %\n",
      "Acc: 86.42\n",
      "Progress: 37.0 %\n",
      "Acc: 86.52\n",
      "Progress: 38.0 %\n",
      "Acc: 86.36\n",
      "Progress: 39.0 %\n",
      "Acc: 86.42\n",
      "Progress: 40.0 %\n",
      "Acc: 86.53999999999999\n",
      "Progress: 41.0 %\n",
      "Acc: 86.48\n",
      "Progress: 42.0 %\n",
      "Acc: 86.58\n",
      "Progress: 43.0 %\n",
      "Acc: 86.52\n",
      "Progress: 44.0 %\n",
      "Acc: 86.52\n",
      "Progress: 45.0 %\n",
      "Acc: 86.5\n",
      "Progress: 46.0 %\n",
      "Acc: 86.52\n",
      "Progress: 47.0 %\n",
      "Acc: 86.48\n",
      "Progress: 48.0 %\n",
      "Acc: 86.56\n",
      "Progress: 49.0 %\n",
      "Acc: 86.58\n",
      "Progress: 50.0 %\n",
      "Acc: 86.56\n",
      "Progress: 51.0 %\n",
      "Acc: 86.44\n",
      "Progress: 52.0 %\n",
      "Acc: 86.58\n",
      "Progress: 53.0 %\n",
      "Acc: 86.52\n",
      "Progress: 54.0 %\n",
      "Acc: 86.53999999999999\n",
      "Progress: 55.0 %\n",
      "Acc: 86.6\n",
      "Progress: 56.0 %\n",
      "Acc: 86.4\n",
      "Progress: 57.0 %\n",
      "Acc: 86.58\n",
      "Progress: 58.0 %\n",
      "Acc: 86.5\n",
      "Progress: 59.0 %\n",
      "Acc: 86.56\n",
      "Progress: 60.0 %\n",
      "Acc: 86.61999999999999\n",
      "Progress: 61.0 %\n",
      "Acc: 86.68\n",
      "Progress: 62.0 %\n",
      "Acc: 86.56\n",
      "Progress: 63.0 %\n",
      "Acc: 86.58\n",
      "Progress: 64.0 %\n",
      "Acc: 86.56\n",
      "Progress: 65.0 %\n",
      "Acc: 86.42\n",
      "Progress: 66.0 %\n",
      "Acc: 86.64\n",
      "Progress: 67.0 %\n",
      "Acc: 86.48\n",
      "Progress: 68.0 %\n",
      "Acc: 86.61999999999999\n",
      "Progress: 69.0 %\n",
      "Acc: 86.7\n",
      "Progress: 70.0 %\n",
      "Acc: 86.72\n",
      "Progress: 71.0 %\n",
      "Acc: 86.44\n",
      "Progress: 72.0 %\n",
      "Acc: 86.68\n",
      "Progress: 73.0 %\n",
      "Acc: 86.6\n",
      "Progress: 74.0 %\n",
      "Acc: 86.42\n",
      "Progress: 75.0 %\n",
      "Acc: 86.74\n",
      "Progress: 76.0 %\n",
      "Acc: 86.44\n",
      "Progress: 77.0 %\n",
      "Acc: 86.58\n",
      "Progress: 78.0 %\n",
      "Acc: 86.56\n",
      "Progress: 79.0 %\n",
      "Acc: 86.58\n",
      "Progress: 80.0 %\n",
      "Acc: 86.48\n",
      "Progress: 81.0 %\n",
      "Acc: 86.7\n",
      "Progress: 82.0 %\n",
      "Acc: 86.64\n",
      "Progress: 83.0 %\n",
      "Acc: 86.44\n",
      "Progress: 84.0 %\n",
      "Acc: 86.64\n",
      "Progress: 85.0 %\n",
      "Acc: 86.61999999999999\n",
      "Progress: 86.0 %\n",
      "Acc: 86.53999999999999\n",
      "Progress: 87.0 %\n",
      "Acc: 86.58\n",
      "Progress: 88.0 %\n",
      "Acc: 86.64\n",
      "Progress: 89.0 %\n",
      "Acc: 86.52\n",
      "Progress: 90.0 %\n",
      "Acc: 86.64\n",
      "Progress: 91.0 %\n",
      "Acc: 86.6\n",
      "Progress: 92.0 %\n",
      "Acc: 86.52\n",
      "Progress: 93.0 %\n",
      "Acc: 86.64\n",
      "Progress: 94.0 %\n",
      "Acc: 86.68\n",
      "Progress: 95.0 %\n",
      "Acc: 86.61999999999999\n",
      "Progress: 96.0 %\n",
      "Acc: 86.58\n",
      "Progress: 97.0 %\n",
      "Acc: 86.6\n",
      "Progress: 98.0 %\n",
      "Acc: 86.61999999999999\n",
      "Progress: 99.0 %\n",
      "Acc: 86.58\n",
      "=================\n",
      "Input Data:\n",
      " [[ 1.          0.47658494  0.68489272 -0.38719208  0.14439528 -0.48006208\n",
      "  -0.35661561 -0.12230543 -0.20952634  0.46786025 -0.68062167 -0.02954153\n",
      "  -0.37024142  0.59064961  0.08730191  0.01181174 -0.38117712  0.13715757\n",
      "  -0.54200133  0.63766734  0.14911943  0.20775825 -0.19508666 -0.33318347\n",
      "   0.32148041 -0.13485122 -0.03518475 -0.06628156  0.01300912  0.17258049\n",
      "   0.06015624  0.31287409  0.13722822 -0.60240723 -0.43362784 -0.52723645\n",
      "  -0.34563592 -0.14500068  0.44311393 -0.33465815  0.12211194 -0.33024851\n",
      "   0.32440944 -0.47582796  0.32165798 -0.4102768  -0.16169009 -0.06168932\n",
      "  -0.40675727  0.36136908  0.74701027  0.31531424  0.25087214  0.07827294\n",
      "   0.39337014 -0.1323906  -0.25251694 -0.51682411 -0.08737517  0.25245648\n",
      "   0.41318594  0.40591197  0.13452502 -0.07690322 -0.12315681  0.21563215\n",
      "  -0.04865804  0.06554736  0.38819134  0.02652829  0.15193495  0.4032012\n",
      "   0.65411257  0.14760619  0.28655485  0.20874653 -0.16917651 -0.02018169\n",
      "   0.34053053  0.33889836  0.28450432 -0.2329402   0.05993733 -0.09703942\n",
      "   0.35718721 -0.13911586 -0.21916568 -0.31938011 -0.7958789  -0.46738992\n",
      "   0.09883716 -0.02716892  0.2422403   0.14352754  0.01108307  0.35642194\n",
      "   0.38347692  0.10913355 -0.00743583  0.23787488 -0.31656975]]\n",
      "Weights 1:\n",
      " [[-0.43505199 -0.13593256 -0.64634793 ... -0.42257764  0.1015498\n",
      "  -0.58974179]\n",
      " [-0.11484953  0.1132265   0.4035375  ... -0.18333319  0.37004175\n",
      "   0.1282771 ]\n",
      " [-1.02395155 -0.91357064 -0.95420417 ... -0.63335977 -0.37504323\n",
      "  -0.60276921]\n",
      " ...\n",
      " [-0.26533043 -0.45661214 -0.23331327 ...  0.385135   -0.18994897\n",
      "  -0.42081288]\n",
      " [-0.02698336  0.20384224 -0.28540205 ... -0.15591166 -0.2390001\n",
      "  -0.51070281]\n",
      " [ 0.92310459  1.43551472  1.39643427 ...  1.60614471  0.75700848\n",
      "   1.24366458]]\n",
      "Hidden Layer:\n",
      " [[1.         0.34271577 0.39771825 0.32562754 0.70919173 0.28304733\n",
      "  0.65297546 0.75797212 0.63692679 0.66523422 0.69936521 0.81764833\n",
      "  0.65272725 0.15577458 0.55723204 0.67809466 0.76112045 0.50988992\n",
      "  0.65558102 0.80728474 0.53549423 0.542546   0.83926762 0.23929702\n",
      "  0.84087315 0.41386827 0.86190599 0.45592671 0.25304037 0.32038525\n",
      "  0.55435532 0.66218866 0.79035771]]\n",
      "Weights 2:\n",
      " [[-4.87627692]\n",
      " [ 0.43270523]\n",
      " [ 0.54880802]\n",
      " [ 0.01087107]\n",
      " [ 0.83344749]\n",
      " [-0.04016311]\n",
      " [ 0.22536021]\n",
      " [ 0.26425983]\n",
      " [ 0.33447574]\n",
      " [ 0.74871378]\n",
      " [ 0.36736502]\n",
      " [-0.41633719]\n",
      " [ 0.48535821]\n",
      " [ 0.32730058]\n",
      " [ 0.24276859]\n",
      " [ 0.74139234]\n",
      " [ 0.10734974]\n",
      " [ 0.35109701]\n",
      " [-0.57322711]\n",
      " [ 0.82178472]\n",
      " [ 0.05399306]\n",
      " [ 0.43136797]\n",
      " [ 0.89807682]\n",
      " [ 0.76077364]\n",
      " [-0.21144435]\n",
      " [ 0.85170403]\n",
      " [ 0.24448544]\n",
      " [-0.04278916]\n",
      " [-0.13828347]\n",
      " [ 0.3582594 ]\n",
      " [ 0.31921091]\n",
      " [ 0.25177127]\n",
      " [ 0.31838017]]\n",
      "Output Layer:\n",
      " [[0.69817435]]\n",
      "=================\n",
      "Acc: 86.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.68"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corremos la red neuronal en base a la data pre-procesada y calculamos el Acc.\n",
    "NN = NeuralNetwork(32,training_data,training_target,test_data,test_target)\n",
    "NN.Train(1000000,0.01)\n",
    "NN.getAccuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
